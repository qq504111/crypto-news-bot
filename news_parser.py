"""–ü–∞—Ä—Å–µ—Ä –∫—Ä–∏–ø—Ç–æ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""

import feedparser
import requests
from datetime import datetime, timedelta
import json
import os
import re
import html
import io
from html.parser import HTMLParser
from news_config import IMPORTANCE_RULES, EXCLUDE_KEYWORDS, MIN_IMPORTANCE_SCORE, RSS_SOURCES, SOURCE_PRIORITY, STOCK_MARKET_THRESHOLD, PUBLISHED_SIMILARITY_THRESHOLD, BATCH_SIMILARITY_THRESHOLD

# OpenAI Integration
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("‚ö†Ô∏è OpenAI not available - Alpha Take will be skipped")


# HTML Stripper –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ summary –æ—Ç —Ç–µ–≥–æ–≤
class MLStripper(HTMLParser):
    """–£–¥–∞–ª—è–µ—Ç HTML —Ç–µ–≥–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    def __init__(self):
        super().__init__()
        self.reset()
        self.strict = False
        self.convert_charrefs = True
        self.text = []
    
    def handle_data(self, d):
        self.text.append(d)
    
    def get_data(self):
        return ''.join(self.text)
    
    def clear(self):
        self.text = []


def parse_all_feeds():
    """–ü–∞—Ä—Å–∏–º –≤—Å–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏"""
    all_news = []
    
    for source_name, config in RSS_SOURCES.items():
        try:
            # –ü–∞—Ä—Å–∏–º RSS (feedparser –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç timeout –∞—Ä–≥—É–º–µ–Ω—Ç)
            feed = feedparser.parse(config['url'])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ feed –≤–∞–ª–∏–¥–Ω—ã–π
            if hasattr(feed, 'bozo') and feed.bozo and not feed.entries:
                print(f"‚úó {source_name}: Invalid RSS feed")
                continue
                
            print(f"‚úì Parsed {source_name}: {len(feed.entries)} entries")
            
            for entry in feed.entries[:10]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                try:
                    # –ü–∞—Ä—Å–∏–º –≤—Ä–µ–º—è —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π
                    if not hasattr(entry, 'published_parsed') or entry.published_parsed is None:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è –µ—Å–ª–∏ –Ω–µ—Ç –º–µ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–∏
                        published = datetime.now()
                    else:
                        published = datetime(*entry.published_parsed[:6])
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ä—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (>12 —á–∞—Å–æ–≤)
                    if datetime.now() - published > timedelta(hours=12):
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
                    if not hasattr(entry, 'title') or not hasattr(entry, 'link'):
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ title –Ω–µ –ø—É—Å—Ç–æ–π
                    if not entry.title or not entry.title.strip():
                        continue
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º summary (–ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü)
                    summary = ''
                    
                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ summary
                    if hasattr(entry, 'summary') and entry.summary:
                        summary = entry.summary
                    elif hasattr(entry, 'description') and entry.description:
                        summary = entry.description
                    elif hasattr(entry, 'content') and entry.content:
                        # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ content
                        if isinstance(entry.content, list) and entry.content:
                            if isinstance(entry.content[0], dict):
                                summary = entry.content[0].get('value', '')
                    elif hasattr(entry, 'subtitle') and entry.subtitle:
                        summary = entry.subtitle
                    
                    # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏ –∏–∑ summary
                    if summary:
                        try:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π stripper
                            stripper = MLStripper()
                            stripper.feed(summary)
                            summary = stripper.get_data().strip()
                        except Exception as e:
                            # Fallback - –ø—Ä–æ—Å—Ç–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ regex
                            summary = re.sub(r'<[^>]+>', '', summary).strip()
                        
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
                        summary = re.sub(r'\n+', ' ', summary)
                        summary = re.sub(r'\s+', ' ', summary)
                        
                        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –ø–µ—Ä–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (—É–±—Ä–∞–ª–∏ –º–∏–Ω–∏–º—É–º 150 —Å–∏–º–≤–æ–ª–æ–≤)
                        if '. ' in summary:
                            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                            sentences = summary.split('. ')
                            if len(sentences) >= 2:
                                summary = '. '.join(sentences[:2]) + '.'
                            elif sentences and sentences[0]:
                                summary = sentences[0]
                                if not summary.endswith('.'):
                                    summary += '...'
                        
                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                        if len(summary) > 300:
                            summary = summary[:297] + '...'
                        
                        # –ï—Å–ª–∏ summary —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (< 20 —Å–∏–º–≤–æ–ª–æ–≤) - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
                        if len(summary) < 20:
                            summary = ''
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∫–∞—Ä—Ç–∏–Ω–∫–∏
                    image_url = None
                    
                    # –í–∞—Ä–∏–∞–Ω—Ç 1: media_content (CoinDesk, The Block)
                    if (hasattr(entry, 'media_content') and 
                        isinstance(entry.media_content, list) and 
                        entry.media_content and
                        isinstance(entry.media_content[0], dict)):
                        image_url = entry.media_content[0].get('url')
                    
                    # –í–∞—Ä–∏–∞–Ω—Ç 2: media_thumbnail
                    if (not image_url and 
                        hasattr(entry, 'media_thumbnail') and 
                        isinstance(entry.media_thumbnail, list) and
                        entry.media_thumbnail and
                        isinstance(entry.media_thumbnail[0], dict)):
                        image_url = entry.media_thumbnail[0].get('url')
                    
                    # –í–∞—Ä–∏–∞–Ω—Ç 3: enclosure
                    if not image_url and hasattr(entry, 'enclosures') and isinstance(entry.enclosures, list):
                        for enclosure in entry.enclosures:
                            if isinstance(enclosure, dict) and enclosure.get('type', '').startswith('image/'):
                                image_url = enclosure.get('href')
                                break
                    
                    # –í–∞—Ä–∏–∞–Ω—Ç 4: –ø–∞—Ä—Å–∏–º –∏–∑ content/summary HTML
                    if not image_url and hasattr(entry, 'content') and isinstance(entry.content, list):
                        try:
                            content_html = entry.content[0].get('value', '') if isinstance(entry.content[0], dict) else ''
                            # –ë–æ–ª–µ–µ –≥–∏–±–∫–∏–π regex - –ª–æ–≤–∏—Ç single –∏ double quotes
                            img_match = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', content_html, re.IGNORECASE)
                            if img_match:
                                image_url = img_match.group(1)
                        except (IndexError, AttributeError, TypeError):
                            pass
                    
                    # –í–∞–ª–∏–¥–∞—Ü–∏—è URL - —Ç–æ–ª—å–∫–æ http/https
                    if image_url:
                        image_url = image_url.strip()
                        if not (image_url.startswith('http://') or image_url.startswith('https://')):
                            image_url = None
                    
                    all_news.append({
                        'title': entry.title.strip(),
                        'link': entry.link,
                        'summary': summary,
                        'image_url': image_url,
                        'published': published.isoformat(),
                        'source': source_name,
                        'source_weight': config['weight_multiplier']
                    })
                except Exception as e:
                    print(f"  ‚ö† Skipping entry from {source_name}: {e}")
                    continue
                    
        except Exception as e:
            print(f"‚úó Error parsing {source_name}: {e}")
    
    return all_news


def calculate_importance(news_item):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏"""
    title = news_item['title'].lower()
    score = 0
    matched_categories = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    for exclude in EXCLUDE_KEYWORDS:
        if exclude in title:
            return 0, ['EXCLUDED']
    
    # –°—á–∏—Ç–∞–µ–º –±–∞–ª–ª—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    for category, rules in IMPORTANCE_RULES.items():
        category_matched = False
        for keyword in rules['keywords']:
            if keyword.lower() in title:
                score += rules['weight']
                if category not in matched_categories:
                    matched_categories.append(category)
                category_matched = True
                break  # –û–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è SEC (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ö)
    if 'sec' in title and 'CRITICAL' not in matched_categories and 'HIGH' not in matched_categories:
        score += 50
        matched_categories.append('HIGH')
    
    # –ë–æ–Ω—É—Å –∑–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ Bitcoin
    if 'bitcoin' in title or re.search(r'\bbtc\b', title):
        score *= 1.3
    
    # –ë–æ–Ω—É—Å –∑–∞ —Ü–∏—Ñ—Ä—ã (–∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞) - —É–ª—É—á—à–µ–Ω–Ω—ã–π regex
    # –õ–æ–≤–∏—Ç: $100M, $1.5B, 50%, $100 million, $1,234,567
    if re.search(r'\$\s*[\d,]+\.?\d*\s*[mbk]?|\$\s*[\d,]+|\d+\.?\d*%', title, re.IGNORECASE):
        score *= 1.2
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    score *= news_item['source_weight']
    
    return round(score), matched_categories


def titles_are_similar(title1, title2, threshold=0.5):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –ø–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—é —Å–ª–æ–≤ (Jaccard similarity)"""
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞
    words1 = set(re.sub(r'[^\w\s]', '', title1.lower()).split())
    words2 = set(re.sub(r'[^\w\s]', '', title2.lower()).split())
    
    # –£–±–∏—Ä–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'as', 'is'}
    words1 = {w for w in words1 if len(w) > 2 and w not in stop_words}
    words2 = {w for w in words2 if len(w) > 2 and w not in stop_words}
    
    if not words1 or not words2:
        return False
    
    # –°—á–∏—Ç–∞–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ (Jaccard similarity)
    intersection = len(words1 & words2)
    union = len(words1 | words2)
    similarity = intersection / union if union > 0 else 0
    
    return similarity >= threshold


def filter_duplicates(news_items):
    """–£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    unique_news = []
    
    for item in news_items:
        duplicate_index = -1
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏ (—Å—Ä–µ–¥–Ω–∏–π –ø–æ—Ä–æ–≥)
        for i, existing in enumerate(unique_news):
            if titles_are_similar(item['title'], existing['title'], BATCH_SIMILARITY_THRESHOLD):
                duplicate_index = i
                break
        
        if duplicate_index >= 0:
            # –ù–∞—à–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç - –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            existing = unique_news[duplicate_index]
            item_priority = SOURCE_PRIORITY.get(item['source'], 999)
            existing_priority = SOURCE_PRIORITY.get(existing['source'], 999)
            
            if item_priority < existing_priority:
                # –ù–æ–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ - –∑–∞–º–µ–Ω—è–µ–º
                print(f"  ‚ö† Duplicate: replacing {existing['source']} with {item['source']}: {item['title'][:50]}...")
                unique_news[duplicate_index] = item
            else:
                # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                print(f"  ‚ö† Duplicate: keeping {existing['source']} over {item['source']}: {item['title'][:50]}...")
        else:
            # –ù–µ –¥—É–±–ª–∏–∫–∞—Ç - –¥–æ–±–∞–≤–ª—è–µ–º
            unique_news.append(item)
    
    return unique_news


def filter_already_published(news_items, published):
    """–§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ (–ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º)"""
    filtered_news = []
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π (—Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ)
    published_titles = []
    published_links = set(published.keys())  # –í—Å–µ —Å—Å—ã–ª–∫–∏ (–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
    
    for link, data in published.items():
        if isinstance(data, dict) and data.get('title'):
            published_titles.append(data['title'])
    
    for item in news_items:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –ü–æ —Å—Å—ã–ª–∫–µ (–±—ã—Å—Ç—Ä–æ)
        if item['link'] in published_links:
            print(f"  ‚ö† Already published (link): {item['title'][:50]}...")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –ü–æ –∑–∞–≥–æ–ª–æ–≤–∫—É (—Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ - –ª–æ–≤–∏–º –≤—Å–µ –ø–æ—Ö–æ–∂–∏–µ)
        is_duplicate = False
        for pub_title in published_titles:
            if titles_are_similar(item['title'], pub_title, PUBLISHED_SIMILARITY_THRESHOLD):
                print(f"  ‚ö† Already published (similar title): {item['title'][:50]}...")
                is_duplicate = True
                break
        
        if not is_duplicate:
            filtered_news.append(item)
    
    return filtered_news


def load_published():
    """–ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
    try:
        if os.path.exists('published_news.json'):
            with open('published_news.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"‚úì Loaded {len(data)} items from published_news.json")
                
                # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ (>7 –¥–Ω–µ–π) –ò –∑–∞–ø–∏—Å–∏ —Å –ø—É—Å—Ç—ã–º title
                week_ago = datetime.now() - timedelta(days=7)
                cleaned_data = {}
                removed_empty_titles = 0
                
                for k, v in data.items():
                    try:
                        # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç: {link: {timestamp, title}}
                        if isinstance(v, dict):
                            # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ —Å –ø—É—Å—Ç—ã–º title (–Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Ö–æ–∂–µ—Å—Ç—å)
                            if not v.get('title') or not v.get('title').strip():
                                removed_empty_titles += 1
                                continue
                                
                            published_date = datetime.fromisoformat(v.get('timestamp', '').replace('Z', '+00:00'))
                            if published_date > week_ago:
                                cleaned_data[k] = v
                        # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç: {link: timestamp} - —É–¥–∞–ª—è–µ–º (–Ω–µ—Ç title)
                        else:
                            removed_empty_titles += 1
                            continue
                    except (ValueError, AttributeError) as e:
                        # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        continue
                
                print(f"‚úì After cleanup: {len(cleaned_data)} items (removed {len(data) - len(cleaned_data)} old, {removed_empty_titles} without titles)")
                return cleaned_data
        else:
            print("‚ÑπÔ∏è published_news.json not found - starting fresh")
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"‚ö† Warning loading published news: {e}")
    
    return {}


def save_published(published):
    """–°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏"""
    try:
        with open('published_news.json', 'w', encoding='utf-8') as f:
            json.dump(published, f, indent=2, ensure_ascii=False)
        print(f"‚úì Saved {len(published)} published items to published_news.json")
    except Exception as e:
        print(f"‚úó Error saving published news: {e}")


def process_image_for_telegram(image_url, source):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É: –æ–±—Ä–µ–∑–∞–µ—Ç watermark –¥–ª—è CoinDesk"""
    
    # –¢–æ–ª—å–∫–æ –¥–ª—è CoinDesk –æ–±—Ä–µ–∑–∞–µ–º watermark
    if source.lower() != 'coindesk':
        return image_url  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º URL –∫–∞–∫ –µ—Å—Ç—å
    
    try:
        from PIL import Image
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É
        response = requests.get(image_url, timeout=10)
        if response.status_code != 200:
            print(f"  ‚ö†Ô∏è Failed to download image for cropping")
            return image_url
        
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É
        img = Image.open(io.BytesIO(response.content))
        width, height = img.size
        
        # –û–±—Ä–µ–∑–∞–µ–º 70px —Å–Ω–∏–∑—É (watermark + –Ω–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ —á—Ç–æ–±—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–∫—Ä—ã—Ç—å –ª–æ–≥–æ—Ç–∏–ø)
        crop_pixels = 70
        if height > crop_pixels:
            img_cropped = img.crop((0, 0, width, height - crop_pixels))
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ BytesIO
            output = io.BytesIO()
            img_cropped.save(output, format='JPEG', quality=95)
            output.seek(0)
            
            print(f"  ‚úì Cropped CoinDesk watermark (removed {crop_pixels}px)")
            return output  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∞–π–ª –æ–±—ä–µ–∫—Ç
        else:
            print(f"  ‚ö†Ô∏è Image too small to crop")
            return image_url
            
    except ImportError:
        print(f"  ‚ö†Ô∏è Pillow not installed - skipping watermark removal")
        return image_url
    except Exception as e:
        print(f"  ‚ö†Ô∏è Error processing image: {e}")
        return image_url


def get_alpha_take(news_item):
    """–ü–æ–ª—É—á–∞–µ–º Alpha Take –æ—Ç OpenAI –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏"""
    
    if not OPENAI_AVAILABLE:
        return None
    
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("  ‚ö†Ô∏è OPENAI_API_KEY not found - skipping Alpha Take")
        return None
    
    try:
        client = OpenAI(api_key=api_key)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º Impact Label
        score = news_item.get('score', 0)
        if score >= 80:
            impact = "HIGH"
        elif score >= 60:
            impact = "MEDIUM"
        else:
            impact = "LOW"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        categories = ', '.join(news_item.get('categories', []))
        summary = news_item.get('summary', '')
        
        system_prompt = """üî• MASTER PROMPT ‚Äî MARKET ALERT (Breaking / Urgent News)

ROLE
You are a real-time crypto market alert system for professional investors.
Your task is to surface time-sensitive events that may impact positioning, volatility, or narratives.
You prioritize speed, clarity, and relevance, not depth.
No opinions, no hype, no speculation.
Audience: US-based, market-literate crypto participants.

ALPHA TAKE ‚Äî ALERT EDITION
Rules:
- 1‚Äì2 sentences max
- No predictions
- No bullish/bearish language
- Focus on why this matters now
- No emojis
- No hashtags
- Factual only

Return ONLY the Alpha Take text, nothing else."""

        user_prompt = f"""News Title: {news_item['title']}

Summary: {summary if summary else 'No summary available'}

Score: {score} | Impact: {impact}
Categories: {categories}
Source: {news_item['source'].upper()}

Generate a concise Alpha Take (1-2 sentences) explaining why this matters now for crypto traders."""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=100,
            temperature=0.3,
            timeout=10.0  # 10 second timeout
        )
        
        alpha_take = response.choices[0].message.content.strip()
        
        if alpha_take and len(alpha_take) > 10:
            print(f"  ‚úì Generated Alpha Take: {alpha_take[:50]}...")
            return alpha_take
        else:
            print(f"  ‚ö†Ô∏è Empty Alpha Take received")
            return None
            
    except Exception as e:
        print(f"  ‚ö†Ô∏è OpenAI error: {e}")
        return None


def format_telegram_message(news_item):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è Telegram"""
    
    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ headers –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    header_map = {
        'CRITICAL': 'üö® BREAKING NEWS',
        'HIGH': 'üî• MARKET ALERT',
        'MARKET_MOVE': 'üìà PRICE ALERT',
        'MEDIUM': 'üì∞ CRYPTO UPDATE'
    }
    
    # –í—ã–±–∏—Ä–∞–µ–º header
    main_category = news_item['categories'][0] if news_item['categories'] else 'MEDIUM'
    header = header_map.get(main_category, 'üì∞ CRYPTO UPDATE')
    
    # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º HTML —Å–∏–º–≤–æ–ª—ã –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∏ summary
    safe_title = html.escape(news_item['title'])
    safe_summary = html.escape(news_item.get('summary', ''))
    
    # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if len(safe_title) > 200:
        safe_title = safe_title[:197] + '...'
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    message = f"{header}\n\n"
    message += f"{safe_title}\n\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º summary –µ—Å–ª–∏ –µ—Å—Ç—å
    if safe_summary:
        message += f"{safe_summary}\n\n"
    
    # –£–±—Ä–∞–ª–∏ Score –∏ Source - —ç—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    
    # –î–æ–±–∞–≤–ª—è–µ–º Alpha Take —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Å—Ç–æ
    alpha_take = news_item.get('alpha_take')
    if alpha_take:
        alpha_section = f"üí° <b>Alpha Take:</b>\n{html.escape(alpha_take)}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ Alpha Take –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è
        if len(message) + len(alpha_section) <= 1024:
            message += alpha_section
        else:
            # –ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –º–µ—Å—Ç–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º Alpha Take
            print(f"  ‚ö†Ô∏è Alpha Take too long for Telegram (message would be {len(message) + len(alpha_section)} chars)")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã (–¥–ª—è caption –ª–∏–º–∏—Ç 1024 —Å–∏–º–≤–æ–ª–∞)
    # –≠—Ç–æ –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ summary —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
    if len(message) > 1024:
        # –û–±—Ä–µ–∑–∞–µ–º –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –ø—Ä–æ–±–µ–ª—É —á—Ç–æ–±—ã –Ω–µ —Ä–µ–∑–∞—Ç—å —Å–ª–æ–≤–æ
        message = message[:1020]
        last_space = message.rfind(' ')
        if last_space > 950:  # –ù–µ –æ–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
            message = message[:last_space] + '...'
        else:
            message = message + '...'
    
    return message


def format_twitter_message(news_item):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è Twitter (280 char limit)"""
    
    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ headers
    header_map = {
        'CRITICAL': 'üö® BREAKING',
        'HIGH': 'üî• ALERT',
        'MARKET_MOVE': 'üìà MARKET',
        'MEDIUM': 'üì∞ NEWS'
    }
    
    main_category = news_item['categories'][0] if news_item['categories'] else 'MEDIUM'
    header = header_map.get(main_category, 'üì∞ NEWS')
    
    title = news_item['title']
    alpha_take = news_item.get('alpha_take')
    
    # Twitter limit: 280 chars (NO link in text, only image)
    base_length = len(header) + 5  # header + spacing
    
    # Try to fit: Header + Title + Alpha Take (NO link)
    if alpha_take:
        alpha_text = f"\n\nüí° {alpha_take}"
        available_for_title = 280 - base_length - len(alpha_text)
        
        if available_for_title > 50:  # Enough space for meaningful title
            if len(title) > available_for_title:
                title = title[:available_for_title-3] + '...'
            tweet = f"{header}\n\n{title}{alpha_text}"
        else:
            # Not enough space - skip Alpha Take
            available_for_title = 280 - base_length
            if len(title) > available_for_title:
                title = title[:available_for_title-3] + '...'
            tweet = f"{header}\n\n{title}"
    else:
        # No Alpha Take - just title
        available_for_title = 280 - base_length
        if len(title) > available_for_title:
            title = title[:available_for_title-3] + '...'
        tweet = f"{header}\n\n{title}"
    
    return tweet


def send_to_telegram(news_items):
    """–ü—É–±–ª–∏–∫—É–µ–º –≤ Telegram"""
    import time
    
    bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
    channel_id = os.getenv('TELEGRAM_CHANNEL_ID')
    
    if not bot_token or not channel_id:
        print("‚ùå Telegram credentials not found")
        return []
    
    published_links = []
    
    for i, item in enumerate(news_items):
        # Rate limiting: –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        if i > 0:
            time.sleep(1)  # 1 —Å–µ–∫—É–Ω–¥–∞ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        
        caption = format_telegram_message(item)
        image_url = item.get('image_url')
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è image_url
        if image_url:
            image_url = image_url.strip()
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π URL
            if not image_url or not (image_url.startswith('http://') or image_url.startswith('https://')):
                image_url = None
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É (–æ–±—Ä–µ–∑–∞–µ–º watermark –¥–ª—è CoinDesk)
        processed_image = None
        if image_url:
            processed_image = process_image_for_telegram(image_url, item['source'])
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—Ç–ø—Ä–∞–≤–∫–∏
            is_file = isinstance(processed_image, io.BytesIO)
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∞ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–µ—Ä–µ–∑ sendPhoto
            if processed_image:
                url = f"https://api.telegram.org/bot{bot_token}/sendPhoto"
                
                if is_file:
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∫ —Ñ–∞–π–ª (–æ–±—Ä–µ–∑–∞–Ω–Ω–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ CoinDesk)
                    files = {'photo': ('image.jpg', processed_image, 'image/jpeg')}
                    data = {
                        'chat_id': channel_id,
                        'caption': caption,
                        'parse_mode': 'HTML'
                    }
                    response = requests.post(url, data=data, files=files, timeout=30)
                else:
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∫ URL (–Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞)
                    payload = {
                        'chat_id': channel_id,
                        'photo': processed_image,
                        'caption': caption,
                        'parse_mode': 'HTML'
                    }
                    response = requests.post(url, json=payload, timeout=10)
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏ - –æ–±—ã—á–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
                payload = {
                    'chat_id': channel_id,
                    'text': caption,
                    'parse_mode': 'HTML',
                    'disable_web_page_preview': True
                }
                response = requests.post(url, json=payload, timeout=10)
            
            if response.status_code == 200:
                published_links.append(item['link'])
                print(f"‚úì Published: {item['title'][:50]}...")
            elif response.status_code == 429:
                # Too Many Requests - –∂–¥–µ–º –∏ –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
                try:
                    retry_after = int(response.json().get('parameters', {}).get('retry_after', 60))
                except (ValueError, json.JSONDecodeError):
                    retry_after = 60  # Fallback –µ—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
                print(f"‚ö† Rate limited, waiting {retry_after} seconds...")
                time.sleep(retry_after)
                
                # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
                if is_file:
                    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ —Ñ–∞–π–ª–µ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏
                    processed_image.seek(0)
                    files = {'photo': ('image.jpg', processed_image, 'image/jpeg')}
                    response = requests.post(url, data=data, files=files, timeout=30)
                else:
                    response = requests.post(url, json=payload, timeout=10)
                
                if response.status_code == 200:
                    published_links.append(item['link'])
                    print(f"‚úì Published (retry): {item['title'][:50]}...")
                else:
                    print(f"‚úó Failed after retry: {response.text}")
            elif response.status_code == 400 and processed_image:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ—à–∏–±–∫–∞ —Å–≤—è–∑–∞–Ω–∞ —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π
                error_text = response.text.lower()
                if any(word in error_text for word in ['photo', 'image', 'media', 'file']):
                    # –¢–æ—á–Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π - –ø—Ä–æ–±—É–µ–º –±–µ–∑ –Ω–µ—ë
                    print(f"‚ö† Image failed, retrying without image...")
                    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
                    payload = {
                        'chat_id': channel_id,
                        'text': caption,
                        'parse_mode': 'HTML',
                        'disable_web_page_preview': True
                    }
                    response = requests.post(url, json=payload, timeout=10)
                    if response.status_code == 200:
                        published_links.append(item['link'])
                        print(f"‚úì Published (without image): {item['title'][:50]}...")
                    else:
                        print(f"‚úó Failed: {response.text[:100]}")
                else:
                    # –û—à–∏–±–∫–∞ –Ω–µ —Å–≤—è–∑–∞–Ω–∞ —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π
                    print(f"‚úó Failed to publish (status 400): {response.text[:100]}")
            else:
                print(f"‚úó Failed to publish (status {response.status_code}): {response.text[:100]}")
                
        except requests.exceptions.Timeout:
            print(f"‚úó Timeout sending to Telegram: {item['title'][:50]}...")
        except requests.exceptions.RequestException as e:
            print(f"‚úó Error sending to Telegram: {e}")
        except Exception as e:
            print(f"‚úó Unexpected error: {e}")
    
    return published_links


def send_to_twitter(news_items):
    """–ü—É–±–ª–∏–∫—É–µ–º –≤ Twitter"""
    import time
    from news_config import TWITTER_ENABLED
    
    if not TWITTER_ENABLED:
        print("‚ÑπÔ∏è Twitter disabled")
        return []
    
    # –ü–æ–ª—É—á–∞–µ–º credentials
    api_key = os.getenv('TWITTER_API_KEY')
    api_secret = os.getenv('TWITTER_API_SECRET')
    access_token = os.getenv('TWITTER_ACCESS_TOKEN')
    access_token_secret = os.getenv('TWITTER_ACCESS_TOKEN_SECRET')
    
    if not all([api_key, api_secret, access_token, access_token_secret]):
        print("‚ùå Twitter credentials not found")
        return []
    
    try:
        import tweepy
        
        # Authenticate
        auth = tweepy.OAuth1UserHandler(
            api_key, api_secret,
            access_token, access_token_secret
        )
        api = tweepy.API(auth)
        client = tweepy.Client(
            consumer_key=api_key,
            consumer_secret=api_secret,
            access_token=access_token,
            access_token_secret=access_token_secret
        )
        
    except ImportError:
        print("‚ùå Tweepy not installed")
        return []
    except Exception as e:
        print(f"‚ùå Twitter auth failed: {e}")
        return []
    
    published_links = []
    
    for i, item in enumerate(news_items):
        # Rate limiting
        if i > 0:
            time.sleep(2)
        
        tweet_text = format_twitter_message(item)
        image_url = item.get('image_url')
        
        try:
            media_id = None
            
            # Upload image if available
            if image_url:
                try:
                    # Download image
                    img_response = requests.get(image_url, timeout=10)
                    if img_response.status_code == 200:
                        # Upload to Twitter
                        import tempfile
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp:
                            tmp.write(img_response.content)
                            tmp_path = tmp.name
                        
                        media = api.media_upload(tmp_path)
                        media_id = media.media_id
                        
                        # Cleanup
                        os.unlink(tmp_path)
                except Exception as e:
                    print(f"‚ö† Twitter image upload failed: {e}")
            
            # Post tweet
            if media_id:
                response = client.create_tweet(text=tweet_text, media_ids=[media_id])
            else:
                response = client.create_tweet(text=tweet_text)
            
            if response.data:
                published_links.append(item['link'])
                print(f"‚úì Tweeted: {item['title'][:50]}...")
            else:
                print(f"‚úó Twitter post failed")
                
        except tweepy.TweepyException as e:
            print(f"‚úó Twitter error: {e}")
        except Exception as e:
            print(f"‚úó Unexpected Twitter error: {e}")
    
    return published_links


def main():
    print("=" * 60)
    print("ü§ñ Crypto News Bot - Starting...")
    print("=" * 60)
    
    # 1. –ü–∞—Ä—Å–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    print("\nüì° Fetching news from sources...")
    all_news = parse_all_feeds()
    print(f"Total news fetched: {len(all_news)}")
    
    # –ö–†–ò–¢–ò–ß–ù–û: –ï—Å–ª–∏ –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —É–ø–∞–ª–∏ - –≤—ã—Ö–æ–¥–∏–º —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
    if len(all_news) == 0:
        print("\n‚ö†Ô∏è WARNING: No news fetched from any source!")
        print("This could indicate:")
        print("  - All RSS sources are down")
        print("  - Network connectivity issues")
        print("  - All news are older than 12 hours")
        print("\nSkipping this run. Will try again on next schedule.")
        return  # –í—ã—Ö–æ–¥–∏–º –±–µ–∑ –æ—à–∏–±–∫–∏ —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å workflow
    
    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ
    published = load_published()
    print(f"Already published (last 7 days): {len(published)}")
    
    # 3. –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ (–ø–æ —Å—Å—ã–ª–∫–µ –ò –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É)
    new_news = filter_already_published(all_news, published)
    print(f"New news items: {len(new_news)}")
    
    # 4. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
    print("\nüéØ Calculating importance scores...")
    scored_news = []
    stock_sources = ['marketwatch', 'yahoo_finance', 'reuters']
    
    for item in new_news:
        score, categories = calculate_importance(item)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        threshold = MIN_IMPORTANCE_SCORE
        if item['source'] in stock_sources:
            threshold = STOCK_MARKET_THRESHOLD  # –í—ã—à–µ –ø–æ—Ä–æ–≥ –¥–ª—è stock news
        
        if score >= threshold:
            item['score'] = score
            item['categories'] = categories
            scored_news.append(item)
    
    print(f"News above threshold: {len(scored_news)}")
    
    # 5. –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_news = filter_duplicates(scored_news)
    print(f"After deduplication: {len(unique_news)}")
    
    # 6. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
    unique_news.sort(key=lambda x: x['score'], reverse=True)
    
    # 7. –ë–µ—Ä–µ–º —Ç–æ–ø-3
    top_news = unique_news[:3]
    
    if top_news:
        print(f"\nüì¢ Publishing top {len(top_news)} news items:")
        for i, item in enumerate(top_news, 1):
            summary_preview = item.get('summary', '')[:50] if item.get('summary') else 'NO SUMMARY'
            print(f"{i}. [{item['score']}] {item['title']}")
            print(f"   Summary: {summary_preview}{'...' if len(item.get('summary', '')) > 50 else ''}")
        
        # 7.5 –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º Alpha Take –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        print(f"\nü§ñ Generating Alpha Takes with OpenAI...")
        for item in top_news:
            alpha_take = get_alpha_take(item)
            if alpha_take:
                item['alpha_take'] = alpha_take
        
        # 8. –ü—É–±–ª–∏–∫—É–µ–º –≤ Telegram –∏ Twitter
        telegram_links = send_to_telegram(top_news)
        twitter_links = send_to_twitter(top_news)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —É—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        published_links = list(set(telegram_links + twitter_links))
        
        # 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ –¢–û–õ–¨–ö–û –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ —É—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª–æ—Å—å
        if published_links:
            for link in published_links:
                # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –Ω–æ–≤–æ—Å—Ç—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
                news_item = next((item for item in top_news if item['link'] == link), None)
                published[link] = {
                    'timestamp': datetime.now().isoformat(),
                    'title': news_item['title'] if news_item else ''
                }
            save_published(published)
            print(f"\n‚úÖ Published: {len(telegram_links)} to Telegram, {len(twitter_links)} to Twitter")
        else:
            print(f"\n‚ö† No news items were successfully published")
    else:
        print("\nüí§ No important news found")
    
    print("=" * 60)


if __name__ == '__main__':
    main()
